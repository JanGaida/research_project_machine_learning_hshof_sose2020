{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Generic badge](https://img.shields.io/badge/License-Properitary-red.svg)](https://github.com/JanGaida/research_project_machine_learning_hshof_sose2020/blob/master/LICENSE.md) <br>\n",
    "[![Active Development](https://img.shields.io/badge/Maintenance%20Level-Actively%20Developed-brightgreen.svg)](https://github.com/JanGaida/research_project_machine_learning_hshof_sose2020/) <br>\n",
    "[![Ask Me Anything !](https://img.shields.io/badge/Ask%20me-anything-1abc9c.svg)](https://github.com/JanGaida/research_project_machine_learning_hshof_sose2020/issues) <br>\n",
    "[![Python 3.7.7](https://img.shields.io/badge/python-3.7.7-blue.svg)](https://www.python.org/downloads/release/python-377/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Logo_fh_hof.svg/2000px-Logo_fh_hof.svg.png\" width=\"300\" style=\"background-color:#FFF\">\n",
    "\n",
    "<font size=\"+4\"><i><u>Q-Learning mit 'Super Mario Bros'</u></i></font>\n",
    "\n",
    "Seminararbeit der Vorlesung **Angewandtes Maschinelles Lernen** an der **Hochschule für angewande Wissenschaften Hof** des **Sommersemesters 2020**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminologie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://www.incompleteideas.net/book/ebook/node28.html\"><img src=\"http://www.incompleteideas.net/book/ebook/figtmp7.png\"/></a>\n",
    "\n",
    "- <u><b>Agent:</b></u> Der Lernern und Entscheidungsfinder.\n",
    "- <u><b>Environment:</b></u> Dort wo der Agent lernt und entscheidet welche Aktionen er durchführt.\n",
    "- <u><b>Action:</b></u> Ein Set von Aktionen die der Agent durchführen kann.\n",
    "- <u><b>State:</b></u> Der Status des Agenten im Enviorment.\n",
    "- <u><b>Reward:</b></u> Für jede Action ausgewählt durch den Agenten berrechnet das Enviorment einen Reward.\n",
    "- <u><b>Policy:</b></u> Die Entscheidungsfindendungs-Funktion (vgl. 'control strategy') des Agenten, welcher eine Set von Situationen zu Aktionen representiert.\n",
    "- <u><b>Value Funktion:</b></u> Eine Set von State's zu realen Zahlen, wo der Wert eines State den longterm Reward ausgehend vom Start des States representiert.\n",
    "- <u><b>Function approximator:</b></u> Bezieht sich auf das Problem des Induzierens einer Funktion aus Trainingsbeispielen. Standard-Approximatoren umfassen Entscheidungsbäume, neuronale Netze und Nähster-Nachbar-Methoden.\n",
    "- <u><b>Markov decision process (MDP):</b></u> Ein Wahrscheinlichkeitsmodell eines sequentiellen Entscheidungsproblems, bei dem Zustände genau wahrgenommen werden können und der aktuelle Zustand und die ausgewählte Aktion eine Wahrscheinlichkeitsverteilung auf zukünftige Zustände bestimmen. Im Wesentlichen hängt das Ergebnis der Anwendung einer Aktion auf einen Status nur von der aktuellen Aktion und dem aktuellen Status ab (und nicht von vorhergehenden Aktionen oder Status).\n",
    "- <u><b>Dynamic programming (DP):</b></u> Ist eine Klasse von Lösungsmethoden zur Lösung sequentieller Entscheidungsprobleme mit einer kompositorischen Kostenstruktur. Richard Bellman war einer der Hauptgründer dieses Ansatzes.\n",
    "- <u><b>Monte Carlo methods:</b></u> Eine Klasse von Methoden zum Erlernen von Wertfunktionen, die den Wert eines Zustands durch Ausführen vieler Versuche ab diesem Zustand schätzt und dann die Gesamtbelohnungen für diese Versuche mittelt.\n",
    "- <u><b>Temporal Difference (TD) algorithms:</b></u> Eine Klasse von Lernmethoden, die auf der Idee basiert, zeitlich aufeinanderfolgende Vorhersagen zu vergleichen. Möglicherweise die grundlegendste Idee beim gesamten Reinforcement Learning.\n",
    "- <u><b>Model:</b></u> Die Ansicht des Agent über das Enviorment, in der Zustands-Aktions-Paare Wahrscheinlichkeitsverteilungen über Zustände zugeordnet werden. Beachten Sie, dass nicht jeder Reinforcement-Learning-Agent ein Modell seiner Umgebung verwendet muss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorbereitungen\n",
    "\n",
    "*Code der das Jupyter-Notebook vorbereit.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T12:01:21.619987Z",
     "start_time": "2020-04-24T12:01:21.615688Z"
    }
   },
   "source": [
    "## Notebook-Styling\n",
    "\n",
    "*Ändert u.a. das Layout des Notebooks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:28.078305Z",
     "start_time": "2020-04-26T21:39:28.063296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width: 80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "_notebook_relative_width = \"80%\" # Wertebereich: 0-100, muss nach HTML-Schema definiert sein\n",
    "display(HTML('<style>.container { width: ' + _notebook_relative_width + ' !important; }</style>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "*Die benötigten Packages wurden aus der bei mir installierten Pip-Liste gebildet.*\n",
    "\n",
    "**Hinweis:** *Es werden* ***mehr als zwingend notwendige*** *Packages installiert.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:28.151075Z",
     "start_time": "2020-04-26T21:39:28.079593Z"
    }
   },
   "outputs": [],
   "source": [
    "# Package-Liste\n",
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:28.228920Z",
     "start_time": "2020-04-26T21:39:28.154298Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installation\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:28.302648Z",
     "start_time": "2020-04-26T21:39:28.231943Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Update \n",
    "#!pip list --outdated --format=freeze | grep -v '^\\-e' | cut -d = -f 1  | xargs -n1 pip install -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:28.821357Z",
     "start_time": "2020-04-26T21:39:28.304974Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Export Pip-Packages nach './requirements.txt'\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konfiguration\n",
    "\n",
    "*Import-Statements & Vorbereitung der Frameworks/Module zusammengefasst.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allgemein\n",
    "\n",
    "*Nicht weiter spezifizierte Import-Statements, bspw. von System-Bibliotheken.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:28.879652Z",
     "start_time": "2020-04-26T21:39:28.824219Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, glob, uuid, shutil\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:28.902272Z",
     "start_time": "2020-04-26T21:39:28.880797Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Rendert den übergebenen Content als HTML-Objekt\n",
    "def html(content):\n",
    "    display(HTML(content))\n",
    "    \n",
    "# Zeigt einen Paragraphen mit der angegeben Farbe (default: Schwarz)\n",
    "def html_p(content, color = 'black', bold = False):\n",
    "    if bold:\n",
    "        html('<p style=\"color:{};\"><b>{}</b></p>'.format(color, content))\n",
    "    else:\n",
    "        html('<p style=\"color:{};\">{}</p>'.format(color, content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:28.984733Z",
     "start_time": "2020-04-26T21:39:28.904914Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color:red;\"><b>'./logs/DQN_1/events.out.tfevents.1587936798.jan-linux' gelöscht.</b></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color:red;\"><b>'./logs/DQN_1'-Ordner gelöscht.</b></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color:red;\"><b>'./logs/DQN_2/events.out.tfevents.1587937159.jan-linux' gelöscht.</b></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color:red;\"><b>'./logs/DQN_2'-Ordner gelöscht.</b></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color:gray;\"><b>'./logs'-Ordner geleert.</b></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logs_base_dir = './logs'\n",
    "\n",
    "if os.path.isdir(logs_base_dir):\n",
    "    for f in glob.glob('{}/*'.format(logs_base_dir)):\n",
    "        if os.path.isdir(f):\n",
    "            for f2 in glob.glob('{}/*'.format(f)):\n",
    "                html_p('\\'{}\\' gelöscht.'.format(f2),'red',True)\n",
    "                os.remove(f2)\n",
    "            html_p('\\'{}\\'-Ordner gelöscht.'.format(f),'red',True)\n",
    "            shutil.rmtree(f)\n",
    "        else:\n",
    "            html_p('\\'{}\\' gelöscht.'.format(f),'red',True)\n",
    "            os.remove(f)\n",
    "            \n",
    "    html_p('\\'{}\\'-Ordner geleert.'.format(logs_base_dir),'gray',True)\n",
    "else:\n",
    "    os.mkdir(logs_base_dir)\n",
    "    html_p('\\'{}\\'-Ordner erstellt.'.format(logs_base_dir),'gray',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T12:17:17.895941Z",
     "start_time": "2020-04-26T12:17:17.890376Z"
    }
   },
   "source": [
    "### Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:29.117527Z",
     "start_time": "2020-04-26T21:39:28.987804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color:green;\"><b>'./records'-Ordner bereits vorhanden.</b></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rec_base_dir = './records'\n",
    "\n",
    "if not os.path.isdir(rec_base_dir):\n",
    "    os.mkdir(rec_base_dir)\n",
    "    html_p('\\'{}\\'-Ordner erstellt.'.format(rec_base_dir),'gray',True)\n",
    "else:\n",
    "    html_p('\\'{}\\'-Ordner bereits vorhanden.'.format(rec_base_dir),'green',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:29.196854Z",
     "start_time": "2020-04-26T21:39:29.119923Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color:green;\"><b>'./models'-Ordner bereits vorhanden.</b></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_base_dir = './models'\n",
    "\n",
    "if not os.path.isdir(model_base_dir):\n",
    "    os.mkdir(model_base_dir)\n",
    "    html_p('\\'{}\\'-Ordner erstellt.'.format(model_base_dir),'gray',True)\n",
    "else:\n",
    "    html_p('\\'{}\\'-Ordner bereits vorhanden.'.format(model_base_dir),'green',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/TensorFlowLogo.svg/1280px-TensorFlowLogo.svg.png\" width=\"150px\"/>\n",
    "\n",
    "*TensorFlow ist ein Framework zur datenstromorientierten Programmierung. Es wird aus Python-Programmen heraus benutzt und ist in Python und C++ implementiert. Populäre Anwendung findet TensorFlow im Bereich des maschinellen Lernens. [...]*\n",
    "\n",
    "[Weiter Informationen](https://pypi.org/project/tensorflow/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:30.272343Z",
     "start_time": "2020-04-26T21:39:29.200165Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/site-assets/images/project-logos/tensorboard-logo-social.png\" width=\"250px\"/>\n",
    "\n",
    "*TensorBoard ist ein Sammlung von Web-Applicationen um TensorFlow zu inspizieren und zu verstehen.*\n",
    "\n",
    "[Weitere Informationen](https://pypi.org/project/tensorboard/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:30.276460Z",
     "start_time": "2020-04-26T21:39:30.273423Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:30.349496Z",
     "start_time": "2020-04-26T21:39:30.277380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 8182), started 3:58:25 ago. (Use '!kill 8182' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6007\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb046fa7810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known TensorBoard instances:\n",
      "  - port 6007: logdir ./logs (started 3:58:25 ago; pid 8182)\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "# Starten einer Tensorboard-Instanz mit Logs & gebindeten Ports\n",
    "%tensorboard --port=6007 --logdir {logs_base_dir}\n",
    "# Auflisten der TensorBoard-Instanzen (resourcen intesiv)\n",
    "notebook.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPyWidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:30.442964Z",
     "start_time": "2020-04-26T21:39:30.351145Z"
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyVirtualDisplay\n",
    "*Ein python wrapper für Xvfb, Xephyr und Xvnc.*\n",
    "\n",
    "[Weiter Informationen](https://pypi.org/project/PyVirtualDisplay/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:30.495991Z",
     "start_time": "2020-04-26T21:39:30.443989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color:red;\"><b>In case of failure in the next Code-Block consider installing python-opengl, xvfb, ffmpeg, x11-utils - Packages via apt-get.\n",
       "Use command 'sudo apt-get install -y python-opengl xvfb ffmpeg x11-utils > /dev/null' inside a terminal.</b></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# videocode dependecies; x11-utils als workaround für 'xdpyinfo'\n",
    "html_p(\"In case of failure in the next Code-Block consider installing python-opengl, xvfb, ffmpeg, x11-utils - Packages via apt-get.\\nUse command 'sudo apt-get install -y python-opengl xvfb ffmpeg x11-utils > /dev/null' inside a terminal.\",'red', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:30.653572Z",
     "start_time": "2020-04-26T21:39:30.498401Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from pyvirtualdisplay import Display as PyVDisplay\n",
    "\n",
    "py_vdisplay = PyVDisplay(visible=0, size=(1400, 900))\n",
    "py_vdisplay.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI-Gym\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/OpenAI_Logo.svg/1920px-OpenAI_Logo.svg.png\" width=\"275px\"/>\n",
    "\n",
    "*OpenAI-Gym ist ein Toolkit für das Entwickeln und Vergleichen von Reinforcement-Learning-Algorithmen.*\n",
    "\n",
    "[Weitere Informationen](https://pypi.org/project/gym/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:30.750274Z",
     "start_time": "2020-04-26T21:39:30.655125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf51274c0074c69b1b171171cdb891b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Logger_lvl', options=('DISABLED', 'DEBUG', 'INFO', 'WARN', 'ERROR'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from gym.wrappers import Monitor\n",
    "from gym import logger as gymlogger\n",
    "\n",
    "gym_loggler_lvl = 'DISABLED'\n",
    "\n",
    "@interact\n",
    "def show_loggler_lvl_param(Logger_lvl=[ 'DISABLED', 'DEBUG', 'INFO', 'WARN', 'ERROR']):\n",
    "    global gym_loggler_lvl\n",
    "    if Logger_lvl == 'DISABLED':\n",
    "        gymlogger.set_level(50)\n",
    "    elif Logger_lvl == 'DEBUG':\n",
    "        gymlogger.set_level(10)\n",
    "    elif Logger_lvl == 'INFO':\n",
    "        gymlogger.set_level(20)\n",
    "    elif Logger_lvl == 'WARN':\n",
    "        gymlogger.set_level(30)\n",
    "    elif Logger_lvl == 'ERROR':\n",
    "        gymlogger.set_level(40)\n",
    "    else:\n",
    "        raise Error('Unbekanntes Logger-Level: \\'{}\\''.format(Logger_lvl))\n",
    "    html_p('Logger-lvl auf {} gesetzt.'.format(Logger_lvl), 'green',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym-Super-Mario-Bros\n",
    "\n",
    "*Eine OpenAI-Gym-Umgebung für Super Mario Bros. und Super Mario Bros. 2 (Lost Levels) auf den NES (Nitendo Entertainment System) mithilfe des nes-py emulators.*\n",
    "\n",
    "[Weitere Informationen](https://pypi.org/project/gym-super-mario-bros/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><thead><tr><th>SuperMarioBros-v0</th><th>SuperMarioBros-v1</th><th>SuperMarioBros-v2</th><th>SuperMarioBros-v3</th><th>SuperMarioBros2-v0</th><th>SuperMarioBros2-v1</th></tr></thead><tbody><tr><td><img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c4717c633d3823dda390ebc21bac34b18e7c22c3/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f323138343436392f34303934383832302d33643135653563322d363833302d313165382d383164342d6563666166666565306131342e706e67\" width=\"175\"></td><td><img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e8eda56caeefcada9af67f43385ef1f48d0ac394/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f323138343436392f34303934383831392d33636666366334382d363833302d313165382d383337332d3866616431363635616337322e706e67\" width=\"175\"></td><td><img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7f53e40eb716be49673cf41fb833486ab3ee104a/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f323138343436392f34303934383831382d33636561303964342d363833302d313165382d386566612d3866333464386230356231312e706e67\" width=\"175\"></td><td><img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/51975e7cc634efb02ed92acfb56368733b25f4d9/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f323138343436392f34303934383831372d33636436363030612d363833302d313165382d386162622d3963656536613331643337372e706e67\" width=\"175\"></td><td><img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0618011a5c6cedb9dba051b8cf134ba51dd0777a/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f323138343436392f34303934383832322d33643362383431322d363833302d313165382d383630622d6166333830326635333733662e706e67\" width=\"175\"></td><td><img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7c42437f4d2f447e192c088eab22739534c2d9be/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f323138343436392f34303934383832312d33643264363161322d363833302d313165382d383738392d6139326537353061613961382e706e67\" width=\"175\"></td></tr></tbody></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:30.796888Z",
     "start_time": "2020-04-26T21:39:30.751216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d87d724bf214dc09e20ac9a9c0dd525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Enviorment', options=('SuperMarioBros-v0', 'SuperMarioBros-v1', 'S…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym_super_mario_bros\n",
    "\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY, SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
    "\n",
    "global gym_enviorment_id\n",
    "global gym_enviorment_movementset\n",
    "\n",
    "gym_enviorment_id = 'SuperMarioBros-v0'\n",
    "gym_enviorment_movementset = 'Simple'\n",
    "\n",
    "@interact\n",
    "def show_env_param(Enviorment=['SuperMarioBros-v0', 'SuperMarioBros-v1', 'SuperMarioBros-v2', 'SuperMarioBros-v3', 'SuperMarioBros2-v0', 'SuperMarioBros2-v1', 'SuperMarioBros2-v2', 'SuperMarioBros2-v3'],\n",
    "                   Movementset=['Complex', 'Simple', 'Only-Right']):\n",
    "    global gym_enviorment_id\n",
    "    global gym_enviorment_movementset\n",
    "    gym_enviorment_id = Enviorment\n",
    "    gym_enviorment_movementset = Movementset\n",
    "    html_p('Gym \\'{}\\' mit Movementset \\'{}\\' eingestellt.'.format(gym_enviorment_id, gym_enviorment_movementset),'green', True)\n",
    "\n",
    "def get_actionset():\n",
    "    global gym_enviorment_movementset\n",
    "    if gym_enviorment_movementset == 'Simple': return SIMPLE_MOVEMENT\n",
    "    elif gym_enviorment_movementset == 'Complex': return COMPLEX_MOVEMENT\n",
    "    elif gym_enviorment_movementset == 'Only-Right': return RIGHT_ONLY\n",
    "    else: raise Error('Unkown Movementset \\\"{}\\\".'.format(gym_enviorment_movementset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable-Baselines\n",
    "\n",
    "*Stable Baselines ist eine Sammlung von optimierten Implementation von Reinforcement-Learning-Algorithmen basierend auf OpenAi.*\n",
    "\n",
    "[Weitere Informationen](https://pypi.org/project/stable-baselines/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><b>Policyies:</b></u>\n",
    "- <b>MlpPolicy</b>: Policy object that implements DQN policy, using a MLP (2 layers of 64)\n",
    "- <b>LnMlpPolicy</b>: Policy object that implements DQN policy, using a MLP (2 layers of 64), with layer normalisation\n",
    "- <b>CnnPolicy</b>: Policy object that implements DQN policy, using a CNN (the nature CNN)\n",
    "- <b>LnCnnPolicy</b>: Policy object that implements DQN policy, using a CNN (the nature CNN), with layer normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:31.576559Z",
     "start_time": "2020-04-26T21:39:30.797941Z"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines.deepq.policies import MlpPolicy, LnMlpPolicy, CnnPolicy, LnCnnPolicy\n",
    "\n",
    "from stable_baselines import DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainings-Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainings-Epochen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:31.595115Z",
     "start_time": "2020-04-26T21:39:31.577650Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color:black;\"><b>Geben Sie eine Anzahl an zu trainierenden Epochen ein (Wertebereich: 1-∞; Default: 1).</b></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74edbf22aa8f42a5bc6c61a44975eab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='1', description='Epochen'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "html_p('Geben Sie eine Anzahl an zu trainierenden Epochen ein (Wertebereich: 1-∞; Default: 1).','black',True)\n",
    "\n",
    "max_amount_of_epoches = 1\n",
    "\n",
    "@interact\n",
    "def show_epoche_picker(Epochen=str(max_amount_of_epoches)):\n",
    "    global max_amount_of_epoches\n",
    "    try:\n",
    "        tmp = int(Epochen)\n",
    "        if tmp > 0: max_amount_of_epoches = tmp\n",
    "        else: max_amount_of_epoches = 1\n",
    "    except ValueError:\n",
    "        max_amount_of_epoches = 1\n",
    "    \n",
    "    html_p('Epochen-Anzahl in Höhe von {} angewandt.'.format(max_amount_of_epoches),'green',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MP4-Aufnahmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:31.689440Z",
     "start_time": "2020-04-26T21:39:31.596053Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color:black;\"><b>Geben Sie an ob Aufnahmen angefertig werden werden sollen.</b></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b356194b7c4c75924d691e6c449a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=True, description='Aufnehmen'), Output()), _dom_classes=('widget-interact…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color:black;\"><b>Geben Sie an wann Aufnahmen angefertig werden sollen (Wertebereich: 0-∞; Default: 0).</b></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a644695801bf43279a2503126f710dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='0', description='Trigger'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gym_enviorment_monitor = True\n",
    "gym_enviorment_monitor_trigger = 0\n",
    "\n",
    "html_p('Geben Sie an ob Aufnahmen angefertig werden werden sollen.', 'black', True)\n",
    "@interact\n",
    "def show_monitor_enabled_param(Aufnehmen = gym_enviorment_monitor):\n",
    "    global gym_enviorment_monitor\n",
    "    gym_enviorment_monitor = Aufnehmen\n",
    "    if gym_enviorment_monitor: html_p('Aktiviert.','green',True)\n",
    "    else: html_p('Deaktiviert.','gray',True)\n",
    "\n",
    "\n",
    "html_p('Geben Sie an wann Aufnahmen angefertig werden sollen (Wertebereich: 0-∞; Default: 0).', 'black', True)\n",
    "\n",
    "@interact\n",
    "def show_monitor_trigger_param(Trigger = str(gym_enviorment_monitor_trigger)):\n",
    "    global gym_enviorment_monitor_trigger\n",
    "    \n",
    "    try:\n",
    "        tmp = int(Trigger)\n",
    "        if tmp > 0: gym_enviorment_monitor_trigger = tmp\n",
    "        else: gym_enviorment_monitor_trigger = 0\n",
    "    except ValueError:\n",
    "        gym_enviorment_monitor_trigger = 0\n",
    "        \n",
    "    if gym_enviorment_monitor_trigger == 0:\n",
    "        html_p('Jeder Run wird aufgenommen.','green',True)\n",
    "    else:\n",
    "        html_p('Jeder {}. Run wird aufgenommen.'.format(gym_enviorment_monitor_trigger),'green',True)\n",
    "        \n",
    "        \n",
    "def should_record(epoche):\n",
    "    global gym_enviorment_monitor_trigger\n",
    "    if gym_enviorment_monitor_trigger == 0: return True\n",
    "    return gym_enviorment_monitor_trigger % epoche == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enviorment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "+--------------+---------------------------------------------+\n",
    "|   Info-Key   |                 Beschreibung                |\n",
    "+--------------+---------------------------------------------+\n",
    "| coins        | Anzahl der von Mario gesammelten Münzen     |\n",
    "| life         | Anzahl der Leben von Mario, {2,1,0,255}     |\n",
    "| score        | Kumulativer in-game Punktestand             |\n",
    "| status       | Mario's Zustand {'small','tall','fireball'} |\n",
    "| flag_get     | True wenn Mario die Flagge erreicht oder aX |\n",
    "| world        | Aktuele Welt {1,2,3,4,5,6,7,8}              |\n",
    "| stage        | Aktuelle Etage {1,2,3,4}                    |\n",
    "| time         | Aktuell übrige Zeit {400 bis 0}             |\n",
    "| x_pos        | Mario's absolute X-Position                 |\n",
    "| x_pos_screen | Mario's relative X-Position                 |\n",
    "| y_pos        | Mario's absolute Y-Position                 |\n",
    "+--------------+---------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:31.763465Z",
     "start_time": "2020-04-26T21:39:31.690414Z"
    }
   },
   "outputs": [],
   "source": [
    "class CollectedReward:\n",
    "    def __init__(self, coins, life, score, time, x_pos, y_pos):\n",
    "        self.coins = coins\n",
    "        self.life = life\n",
    "        self.score = score\n",
    "        self.time = time\n",
    "        self.x_pos = x_pos\n",
    "        self.y_pos = y_pos\n",
    "\n",
    "class SuperMarioBrosEnviorment(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self):\n",
    "        super(SuperMarioBrosEnviorment, self).__init__()\n",
    "        # Init Enviorment\n",
    "        self._init_enviorment()\n",
    "    \n",
    "    \"\"\"Initalizes the Enviorment\"\"\"\n",
    "    def _init_enviorment(self):\n",
    "        global gym_enviorment_id\n",
    "        global gym_enviorment_monitor\n",
    "        \n",
    "        # Init\n",
    "        env = gym.make(gym_enviorment_id)\n",
    "        # Wrapp JoypadSpace\n",
    "        env = JoypadSpace(env, get_actionset())\n",
    "\n",
    "        # Further\n",
    "        self.action_space = env.action_space\n",
    "        self.observation_space = env.observation_space\n",
    "        \n",
    "        self._latest_reward = None\n",
    "        \n",
    "        # Finally\n",
    "        self._env = env\n",
    "    \n",
    "    \"\"\"Wrapped das Enviorment mit dem Monitor\"\"\"\n",
    "    def wrap_monitor(self):\n",
    "        global rec_base_dir\n",
    "        \n",
    "        # Wrapp Monitor\n",
    "        if gym_enviorment_monitor: \n",
    "            self._env = Monitor( self._env,'{}/run_{}__{}'.format(rec_base_dir, datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\"), uuid.uuid4()),\n",
    "                          video_callable = lambda episode_id: should_record((episode_id - 1)), force = False )\n",
    " \n",
    "    \"\"\"Berrechnet den Reward\"\"\"\n",
    "    def calculate_reward(self, info, gym_reward, done):\n",
    "        # the reward\n",
    "        reward = 0\n",
    "        \n",
    "        # grab vars\n",
    "        coins = info['coins']\n",
    "        life = info['life']\n",
    "        score = info['score']\n",
    "        time = info['time']\n",
    "        x_pos = info['x_pos']\n",
    "        y_pos = info['y_pos']\n",
    "    \n",
    "        if not self._latest_reward == None:\n",
    "            l = self._latest_reward\n",
    "            \n",
    "            # Coinbonus\n",
    "            reward += (coins - l.coins) * 5 # {0, 5}\n",
    "            # Lifepenality\n",
    "            reward += (life - l.life) * -100 # {0, -100}\n",
    "            # Scorebonus\n",
    "            reward += (score - l.score) * .02 # {2~20}\n",
    "            # Timepenality\n",
    "            timedif = (l.time - time)\n",
    "            if timedif == 1: reward += -3 # {0, -3}\n",
    "            reward += -1 # {0, -1}\n",
    "            # Progresspenality\n",
    "            if (x_pos - l.x_pos) == 0: # {0, -5}\n",
    "                reward += -5\n",
    "            # Progressbonus\n",
    "            if not (x_pos - l.x_pos == 0) and not (y_pos - l.y_pos == 0): # {0, 2}\n",
    "                reward += 2\n",
    "        \n",
    "        # Warning: Disable this statement else sysout will be flooded.\n",
    "        if reward > 100 or reward < -100:\n",
    "            print('| Coins: {:0>7d} | Life: {:0>3d} | Score: {:0>7d} | Time: {:0>3d} | X_pos: {:0>7d} | Y_pos: {:0>7d} | My_Reward: --- |'.format(l.coins,l.life,l.score,l.time,l.x_pos,l.y_pos))\n",
    "            print('| Coins: {:0>7d} | Life: {:0>3d} | Score: {:0>7d} | Time: {:0>3d} | X_pos: {:0>7d} | Y_pos: {:0>7d} | My_Reward: {} |\\n'.format(coins,life,score,time,x_pos,y_pos,reward))\n",
    "        \n",
    "        # Remeber latest reward\n",
    "        self._latest_reward = CollectedReward(coins, life, score, time, x_pos, y_pos)\n",
    "        \n",
    "        # clip reward {-150 <-> 150}\n",
    "        if reward >= 0: reward = min(reward, 150)\n",
    "        else: reward = max(reward, -150)\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    \"\"\"Die Step-Methode eines Enviorments\"\"\"\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self._env.step(action)\n",
    "        return observation, self.calculate_reward(info, reward, done), done, info\n",
    "    \n",
    "    \"\"\"Die Reset-Methode eines Enviorments\"\"\"\n",
    "    def reset(self):\n",
    "        return self._env.reset()\n",
    "    \n",
    "    \"\"\"Die Render-Methode eines Enviorments\"\"\"\n",
    "    def render(self, mode='human'):\n",
    "        return self._env.render(mode=mode)\n",
    "    \n",
    "    \"\"\"Die Close-Methode eines Enviorments\"\"\"\n",
    "    def close (self):\n",
    "        return self._env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:31.871860Z",
     "start_time": "2020-04-26T21:39:31.766484Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\n",
    "class Hyperparameter:\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self, rl_algo):\n",
    "        # Load Default-Parameters\n",
    "        if rl_algo == 'DQN': \n",
    "            self._default_dqn()\n",
    "            self._default_dqn_learning()\n",
    "        else: raise Error('Undefined Hyperparameter for RL-Algorithm \\\"{}\\\".'.format(rl_algo))\n",
    "        \n",
    "        # Apply Alog-Id\n",
    "        self.rl_algo = rl_algo\n",
    "        \n",
    "        # Apply Tensorboard-Configuration\n",
    "        self._apply_tensorboard()\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "    def _apply_tensorboard(self):\n",
    "        global logs_base_dir\n",
    "        \n",
    "        # (str) the log location for tensorboard (if None, no logging)\n",
    "        self.tensorboard_log = logs_base_dir\n",
    "        # (bool) enable additional logging when using tensorboard WARNING: this logging can take a lot of space quickly\n",
    "        self.full_tensorboard_log = False \n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "    def _default_dqn_learning(self):\n",
    "        # (int) The total number of samples to train on\n",
    "        self.total_timesteps = 1000\n",
    "        # (Union[callable, [callable], BaseCallback]) function called at every steps with state of the algorithm. It takes the local and global variables. If it returns False, training is aborted. When the callback inherits from BaseCallback, you will have access to additional stages of the training (training start/end), please read the documentation for more details.\n",
    "        self.callback = None\n",
    "        # (int) The number of timesteps before logging.\n",
    "        self.log_interval = 100\n",
    "        # (str) the name of the run for tensorboard log\n",
    "        self.tb_log_name = 'DQN'\n",
    "        # (bool) whether or not to reset the current timestep number (used in logging)\n",
    "        self.reset_num_timesteps = False\n",
    "        \n",
    "    \"\"\"\"\"\"\n",
    "    def _default_dqn(self):\n",
    "        # (float) discount factor\n",
    "        self.gamma = 0.95                \n",
    "        # (float) learning rate for adam optimizer\n",
    "        self.learning_rate = 0.005                   \n",
    "        # (int) size of the replay buffer\n",
    "        self.buffer_size = 50000            \n",
    "        # (float) fraction of entire training period over which the exploration rate is annealed\n",
    "        self.exploration_fraction = 0.2         \n",
    "        # (float) final value of random action probability\n",
    "        self.exploration_final_eps = 0.2         \n",
    "        # (float) initial value of random action probability\n",
    "        self.exploration_initial_eps = 1.0                        \n",
    "        # (int) update the model every `train_freq` steps. set to None to disable printing\n",
    "        self.train_freq = 50                       \n",
    "        # (int) size of a batched sampled from replay buffer for training\n",
    "        self.batch_size = 32                       \n",
    "        # (bool) Whether to enable Double-Q learning or not.\n",
    "        self.double_q = False                \n",
    "        # (int) how many steps of the model to collect transitions for before learning starts\n",
    "        self.learning_starts = 100      \n",
    "        # (int) update the target network every `target_network_update_freq` steps.\n",
    "        self.target_network_update_freq = 500            \n",
    "        # (bool) if True prioritized replay buffer will be used.\n",
    "        self.prioritized_replay = False\n",
    "        # (float) alpha parameter for prioritized replay buffer. It determines how much prioritization is used, with alpha=0 corresponding to the uniform case.\n",
    "        self.prioritized_replay_alpha = 0.6          \n",
    "        # (float) initial value of beta for prioritized replay buffer      \n",
    "        self.prioritized_replay_beta0 = 0.4  \n",
    "        # (int) number of iterations over which beta will be annealed from initial value to 1.0. If set to None equals to max_timesteps.\n",
    "        self.prioritized_replay_beta_iters = None        \n",
    "        # (float) epsilon to add to the TD errors when updating priorities.\n",
    "        self.prioritized_replay_eps = 1e-06                   \n",
    "        # (bool) Whether or not to apply noise to the parameters of the policy.\n",
    "        self.param_noise = False                  \n",
    "        # (int) The number of threads for TensorFlow operations If None, the number of cpu of the current machine will be used.\n",
    "        self.n_cpu_tf_sess = None                           \n",
    "        # (int) the verbosity level: 0 none, 1 training information, 2 tensorflow debug\n",
    "        self.verbose = 0              \n",
    "        # (bool) Whether or not to build the network at the creation of the instance\n",
    "        self._init_setup_model = True\n",
    "        # (int) Seed for the pseudo-random generators (python, numpy, tensorflow). If None (default), use random seed. Note that if you want completely deterministicresults, you must set `n_cpu_tf_sess` to 1.\n",
    "        self.seed = None                           \n",
    "        # policy kw-args\n",
    "        self.policy_kwargs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T21:39:31.983459Z",
     "start_time": "2020-04-26T21:39:31.873915Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\n",
    "class Agent:\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self, env, policy, hyp):\n",
    "        # The Policy used\n",
    "        self._policy = policy\n",
    "        # The Enviorment used\n",
    "        self._env = env\n",
    "        # Setup Hyperparameters\n",
    "        self._hyperparameters = hyp\n",
    "        self._setup_model(hyp)\n",
    "        \n",
    "    \"\"\"Decides which model-build-method will be triggered\"\"\"\n",
    "    def _setup_model(self, hyp):\n",
    "        rl_algo = hyp.rl_algo\n",
    "        if rl_algo == 'DQN': self._build_dqn_model(hyp)\n",
    "        #elif rl_aglo == .....\n",
    "        else: raise Error('Undefined RL-Algorithm \\\"{}\\\".'.format(rl_algo))\n",
    "    \n",
    "    \"\"\"Builds a DQN-Model with given Hyperparameters\"\"\" \n",
    "    def _build_dqn_model(self, hyp):\n",
    "        self._model = DQN(\n",
    "            self._policy, self._env,\n",
    "            # Parameters from Hyperparameter-Obj\n",
    "            gamma=hyp.gamma, learning_rate=hyp.learning_rate, buffer_size=hyp.buffer_size, exploration_fraction=hyp.exploration_fraction,\n",
    "            exploration_final_eps=hyp.exploration_final_eps, exploration_initial_eps=hyp.exploration_initial_eps, train_freq=hyp.train_freq, batch_size=hyp.batch_size, double_q=hyp.double_q,\n",
    "            learning_starts=hyp.learning_starts, target_network_update_freq=hyp.target_network_update_freq, prioritized_replay=hyp.prioritized_replay,\n",
    "            prioritized_replay_alpha=hyp.prioritized_replay_alpha, prioritized_replay_beta0=hyp.prioritized_replay_beta0, prioritized_replay_beta_iters=hyp.prioritized_replay_beta_iters,\n",
    "            prioritized_replay_eps=hyp.prioritized_replay_eps, param_noise=hyp.param_noise, n_cpu_tf_sess=hyp.n_cpu_tf_sess, verbose=hyp.verbose, tensorboard_log=hyp.tensorboard_log,\n",
    "            _init_setup_model=hyp._init_setup_model, policy_kwargs=hyp.policy_kwargs, full_tensorboard_log=hyp.full_tensorboard_log, seed=hyp.seed\n",
    "        )\n",
    "    \n",
    "    \"\"\"Returns the Path where this model is saved\"\"\"\n",
    "    def model_path(self):\n",
    "        global model_base_dir\n",
    "        return '{}/{}'.format(model_base_dir,self._hyperparameters.rl_algo)\n",
    "    \n",
    "    \"\"\"Saves the Model\"\"\"\n",
    "    def save_model(self):\n",
    "        self._model.save(self.model_path())\n",
    "        \n",
    "    \"\"\"Loads the Model\"\"\"\n",
    "    def load_model(self):\n",
    "        self._model.load(self.model_path())\n",
    "        \n",
    "    \"\"\"Trains the Model\"\"\"\n",
    "    def train(self, total_timesteps = None):\n",
    "        hyp = self._hyperparameters\n",
    "        model = self._model\n",
    "        # Use default if needed\n",
    "        if total_timesteps == None: total = hyp.total_timesteps\n",
    "        else: total = total_timesteps\n",
    "        # Delegate\n",
    "        return model.learn(total_timesteps=total,callback=hyp.callback, log_interval=hyp.log_interval,\n",
    "                          tb_log_name=hyp.tb_log_name,reset_num_timesteps=hyp.reset_num_timesteps)\n",
    "    \n",
    "    \"\"\"Plays the Enviorment\"\"\"\n",
    "    def play(self, times):\n",
    "        # Prepare\n",
    "        env = self._env\n",
    "        model = self._model\n",
    "\n",
    "        # Main loop\n",
    "        for e in range(times):\n",
    "        \n",
    "            obs = env.reset()\n",
    "            \n",
    "            # In Run\n",
    "            while True:\n",
    "                action, _states = model.predict(obs)\n",
    "                obs, reward, done, info = env.step(action)\n",
    "\n",
    "                env.render()\n",
    "                \n",
    "                if done or info['flag_get']:\n",
    "                    break\n",
    "        # Finished\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-26T21:39:28.068Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/anaconda3/lib/python3.7/site-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
     ]
    }
   ],
   "source": [
    "env = SuperMarioBrosEnviorment()\n",
    "hyp = Hyperparameter('DQN')\n",
    "agent = Agent(env, MlpPolicy, hyp)\n",
    "\n",
    "#agent.load_model()\n",
    "agent.train(50000)\n",
    "#agent.save_model()\n",
    "\n",
    "env.wrap_monitor()\n",
    "\n",
    "agent.play(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "357px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
