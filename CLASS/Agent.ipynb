{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Logo_fh_hof.svg/2000px-Logo_fh_hof.svg.png\" width=\"250\" style=\"background-color:#FFF\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <font size=\"+4\"><i><u>Q-Learning mit 'Super Mario Bros'</u></i></font><br><br>\n",
    "    <font>Seminararbeit der Vorlesung <b>Angewandtes Maschinelles Lernen</b> an der <b>Hochschule für angewande Wissenschaften Hof</b> des <b>Sommersemesters 2020</b>.</font>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Inhaltsverzeichnis<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Parameter-&amp;-Hilfsunktionen\" data-toc-modified-id=\"Parameter-&amp;-Hilfsunktionen-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Parameter &amp; Hilfsunktionen</a></span></li><li><span><a href=\"#Agent\" data-toc-modified-id=\"Agent-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Agent</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T22:18:45.059019Z",
     "start_time": "2020-04-27T22:18:44.734413Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter & Hilfsunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T22:20:26.173579Z",
     "start_time": "2020-04-27T22:20:26.155042Z"
    }
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(5e3)  # replay buffer size\n",
    "BATCH_SIZE = 256         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 10        # how often to update the network\n",
    "\n",
    "_agent_device = get_torch_device()\n",
    "\n",
    "print(\"AGENT-Parameter initialisiert ...\\n... BUFFER_SIZE: {} ...\\n... BATCH_SIZE: {} ...\\n... GAMMA: {} ...\\n... TAU: {} ...\\n... LR: {} ...\\n... UPDATE_EVERY: {} ...\".format(BUFFER_SIZE, BATCH_SIZE, GAMMA, TAU, LR, UPDATE_EVERY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T22:18:45.109600Z",
     "start_time": "2020-04-27T22:18:44.750Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, model, state_size, action_size, seed=42, ddqn=False, priority=False):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.ddqn = ddqn\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = model(state_size[0], action_size, seed).to(_agent_device)\n",
    "        self.qnetwork_target = model(state_size[0], action_size, seed).to(_agent_device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = Memory(state_size, (action_size,), BUFFER_SIZE, BATCH_SIZE)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences, idx = self.memory.sample()\n",
    "                e = self.learn(experiences)\n",
    "                self.memory.update_error(e, idx)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(_agent_device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "          \n",
    "    def update_error(self):\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(get_all=True)\n",
    "        with torch.no_grad():\n",
    "            if self.ddqn:\n",
    "                old_val = self.qnetwork_local(states).gather(-1, actions)\n",
    "                actions = self.qnetwork_local(next_states).argmax(-1, keepdim=True)\n",
    "                maxQ = self.qnetwork_target(next_states).gather(-1, actions)\n",
    "                target = rewards+GAMMA*maxQ*(1-dones)\n",
    "            else: # Normal DQN\n",
    "                maxQ = self.qnetwork_target(next_states).max(-1, keepdim=True)[0]\n",
    "                target = rewards+GAMMA*maxQ*(1-dones)\n",
    "                old_val = self.qnetwork_local(states).gather(-1, actions)\n",
    "            e = old_val - target\n",
    "            self.memory.update_error(e)\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ## compute and minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        if self.ddqn:\n",
    "            old_val = self.qnetwork_local(states).gather(-1, actions)\n",
    "            with torch.no_grad():\n",
    "                next_actions = self.qnetwork_local(next_states).argmax(-1, keepdim=True)\n",
    "                maxQ = self.qnetwork_target(next_states).gather(-1, next_actions)\n",
    "                target = rewards+GAMMA*maxQ*(1-dones)\n",
    "        else: # Normal DQN\n",
    "            with torch.no_grad():\n",
    "                maxQ = self.qnetwork_target(next_states).max(-1, keepdim=True)[0]\n",
    "                target = rewards+GAMMA*maxQ*(1-dones)\n",
    "            old_val = self.qnetwork_local(states).gather(-1, actions)   \n",
    "        \n",
    "        loss = F.mse_loss(old_val, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU) \n",
    "        \n",
    "        return old_val - target\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Inhaltsverzeichnis",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
